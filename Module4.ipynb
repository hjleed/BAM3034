{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "99f9a16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall Module3 and \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7723122f",
   "metadata": {},
   "source": [
    "#  Recall Module3 and \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1d80d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary: {'College', 'the', 'made', 'to', 'Ottawa', 'Lambton', 'decision', 'campus.', 'has', 'close'}\n",
      "Word to Index Mapping: {'College': 0, 'the': 1, 'made': 2, 'to': 3, 'Ottawa': 4, 'Lambton': 5, 'decision': 6, 'campus.': 7, 'has': 8, 'close': 9}\n",
      "One-Hot Encoded Matrix:\n",
      "Lambton: [0, 0, 0, 0, 0, 1, 0, 0, 0, 0]\n",
      "College: [1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "has: [0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "made: [0, 0, 1, 0, 0, 0, 0, 0, 0, 0]\n",
      "the: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "decision: [0, 0, 0, 0, 0, 0, 1, 0, 0, 0]\n",
      "to: [0, 0, 0, 1, 0, 0, 0, 0, 0, 0]\n",
      "close: [0, 0, 0, 0, 0, 0, 0, 0, 0, 1]\n",
      "the: [0, 1, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "Ottawa: [0, 0, 0, 0, 1, 0, 0, 0, 0, 0]\n",
      "campus.: [0, 0, 0, 0, 0, 0, 0, 1, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "def one_hot_encode(text):\n",
    "    words = text.split()\n",
    "    vocabulary = set(words)\n",
    "    word_to_index = {word: i for i, word in enumerate(vocabulary)}\n",
    "    one_hot_encoded = []\n",
    "    for word in words:\n",
    "        one_hot_vector = [0] * len(vocabulary)\n",
    "        one_hot_vector[word_to_index[word]] = 1\n",
    "        one_hot_encoded.append(one_hot_vector)\n",
    "    return one_hot_encoded, word_to_index, vocabulary\n",
    "\n",
    "example_text = \"Lambton College has made the decision to close the Ottawa campus.\"\n",
    "\n",
    "\n",
    "one_hot_encoded, word_to_index, vocabulary = one_hot_encode(example_text)\n",
    "\n",
    "print(\"Vocabulary:\", vocabulary)\n",
    "print(\"Word to Index Mapping:\", word_to_index)\n",
    "print(\"One-Hot Encoded Matrix:\")\n",
    "for word, encoding in zip(example_text.split(), one_hot_encoded):\n",
    "    print(f\"{word}: {encoding}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe69cedf",
   "metadata": {},
   "source": [
    "# Module 4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5dc61ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module4 - \n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82ef64db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unigrams: [('Lambton',), ('College',), ('has',), ('more',), ('than',), ('70',), ('post-secondary',), ('programs',), ('and',), ('apprenticeships',), (',',), ('academic',), ('upgrading',), (',',), ('post-graduate',), (',',), ('part-time',), ('and',), ('training',), ('programs',), ('.',), ('Recently',), (',',), ('the',), ('College',), ('has',), ('made',), ('the',), ('decision',), ('to',), ('close',), ('the',), ('Ottawa',), ('campus',), ('.',), ('Students',), ('will',), ('have',), ('the',), ('opportunity',), ('to',), ('complete',), ('their',), ('programs',), ('.',), ('Its',), ('physical',), ('presence',), ('at',), ('the',), ('Ottawa',), ('campus',), ('will',), ('remain',), ('until',), ('December',), ('31',), (',',), ('2026',), ('.',)]\n",
      "Bigrams: [('Lambton', 'College'), ('College', 'has'), ('has', 'more'), ('more', 'than'), ('than', '70'), ('70', 'post-secondary'), ('post-secondary', 'programs'), ('programs', 'and'), ('and', 'apprenticeships'), ('apprenticeships', ','), (',', 'academic'), ('academic', 'upgrading'), ('upgrading', ','), (',', 'post-graduate'), ('post-graduate', ','), (',', 'part-time'), ('part-time', 'and'), ('and', 'training'), ('training', 'programs'), ('programs', '.'), ('.', 'Recently'), ('Recently', ','), (',', 'the'), ('the', 'College'), ('College', 'has'), ('has', 'made'), ('made', 'the'), ('the', 'decision'), ('decision', 'to'), ('to', 'close'), ('close', 'the'), ('the', 'Ottawa'), ('Ottawa', 'campus'), ('campus', '.'), ('.', 'Students'), ('Students', 'will'), ('will', 'have'), ('have', 'the'), ('the', 'opportunity'), ('opportunity', 'to'), ('to', 'complete'), ('complete', 'their'), ('their', 'programs'), ('programs', '.'), ('.', 'Its'), ('Its', 'physical'), ('physical', 'presence'), ('presence', 'at'), ('at', 'the'), ('the', 'Ottawa'), ('Ottawa', 'campus'), ('campus', 'will'), ('will', 'remain'), ('remain', 'until'), ('until', 'December'), ('December', '31'), ('31', ','), (',', '2026'), ('2026', '.')]\n",
      "Trigrams: [('Lambton', 'College', 'has'), ('College', 'has', 'more'), ('has', 'more', 'than'), ('more', 'than', '70'), ('than', '70', 'post-secondary'), ('70', 'post-secondary', 'programs'), ('post-secondary', 'programs', 'and'), ('programs', 'and', 'apprenticeships'), ('and', 'apprenticeships', ','), ('apprenticeships', ',', 'academic'), (',', 'academic', 'upgrading'), ('academic', 'upgrading', ','), ('upgrading', ',', 'post-graduate'), (',', 'post-graduate', ','), ('post-graduate', ',', 'part-time'), (',', 'part-time', 'and'), ('part-time', 'and', 'training'), ('and', 'training', 'programs'), ('training', 'programs', '.'), ('programs', '.', 'Recently'), ('.', 'Recently', ','), ('Recently', ',', 'the'), (',', 'the', 'College'), ('the', 'College', 'has'), ('College', 'has', 'made'), ('has', 'made', 'the'), ('made', 'the', 'decision'), ('the', 'decision', 'to'), ('decision', 'to', 'close'), ('to', 'close', 'the'), ('close', 'the', 'Ottawa'), ('the', 'Ottawa', 'campus'), ('Ottawa', 'campus', '.'), ('campus', '.', 'Students'), ('.', 'Students', 'will'), ('Students', 'will', 'have'), ('will', 'have', 'the'), ('have', 'the', 'opportunity'), ('the', 'opportunity', 'to'), ('opportunity', 'to', 'complete'), ('to', 'complete', 'their'), ('complete', 'their', 'programs'), ('their', 'programs', '.'), ('programs', '.', 'Its'), ('.', 'Its', 'physical'), ('Its', 'physical', 'presence'), ('physical', 'presence', 'at'), ('presence', 'at', 'the'), ('at', 'the', 'Ottawa'), ('the', 'Ottawa', 'campus'), ('Ottawa', 'campus', 'will'), ('campus', 'will', 'remain'), ('will', 'remain', 'until'), ('remain', 'until', 'December'), ('until', 'December', '31'), ('December', '31', ','), ('31', ',', '2026'), (',', '2026', '.')]\n"
     ]
    }
   ],
   "source": [
    "#N-gram\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Sample sentence\n",
    "# sentence = \"I love programming in Python\"\n",
    "sentence =\"Lambton College has more than 70 post-secondary programs and apprenticeships, academic upgrading, post-graduate, part-time and training programs. Recently, the College has made the decision to close the Ottawa campus. Students will have the opportunity to complete their programs. Its physical presence at the Ottawa campus will remain until December 31, 2026. \"\n",
    "\n",
    "# Tokenize the sentence into words\n",
    "tokens = word_tokenize(sentence)\n",
    "# Function to generate N-grams\n",
    "def generate_ngrams(tokens, n):\n",
    "    return list(ngrams(tokens, n))\n",
    "# Generate Unigrams\n",
    "unigrams = generate_ngrams(tokens, 1)\n",
    "print(\"Unigrams:\", unigrams)\n",
    "# Generate Bigrams\n",
    "bigrams = generate_ngrams(tokens, 2)\n",
    "print(\"Bigrams:\", bigrams)\n",
    "# Generate Trigrams\n",
    "trigrams = generate_ngrams(tokens, 3)\n",
    "print(\"Trigrams:\", trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a1257e89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (manual): 0.5\n"
     ]
    }
   ],
   "source": [
    "# Cosine similarities\n",
    "import numpy as np\n",
    "def cosine_similarity_manual(vec1, vec2):\n",
    "    \"\"\"\n",
    "     This function .Bla Bla\n",
    "    \"\"\"\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    # Compute magnitudes (norms) of the vectors\n",
    "    magnitude_vec1 = np.linalg.norm(vec1)\n",
    "    magnitude_vec2 = np.linalg.norm(vec2)\n",
    "    # Avoid division by zero\n",
    "    if magnitude_vec1 == 0 or magnitude_vec2 == 0:\n",
    "        return 0\n",
    "    # Compute cosine similarity\n",
    "    return dot_product / (magnitude_vec1 * magnitude_vec2)\n",
    "# Example vectors\n",
    "vector1 = [1, 1, 1, 1, 0, 0]\n",
    "vector2 = [1, 1, 0, 0, 1, 1]\n",
    "similarity = cosine_similarity_manual(vector1, vector2)\n",
    "print(f\"Cosine Similarity (manual): {similarity}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b860765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine Similarity (scikit-learn): 0.5\n"
     ]
    }
   ],
   "source": [
    "#Implementation cosine similarity: Using scikit-learn\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "# Example vectors\n",
    "vector1 = np.array([1, 1, 1, 1, 0, 0]).reshape(1, -1) # Reshape to 2D\n",
    "vector2 = np.array([1, 1, 0, 0, 1, 1]).reshape(1, -1) # Reshape to 2D\n",
    "# Compute cosine similarity\n",
    "similarity = cosine_similarity(vector1, vector2)\n",
    "print(f\"Cosine Similarity (scikit-learn): {similarity[0][0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6356460a",
   "metadata": {},
   "source": [
    "Models can’t work with raw text.\n",
    "So we convert words into vectors (embeddings) where:\n",
    "\n",
    "- similar words → nearby vectors\n",
    "\n",
    "- relationships show up as geometry"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf8fb3c",
   "metadata": {},
   "source": [
    "##  Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec80aa",
   "metadata": {},
   "source": [
    "Word2Vec is a word embedding technique in natural language processing (NLP) that allows words to be represented as vectors in a continuous vector space. Researchers at Google developed word2Vec that maps words to high-dimensional vectors to capture the semantic relationships between words. It works on the principle that words with similar meanings should have similar vector representations. Word2Vec utilizes two architectures:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972e58f8",
   "metadata": {},
   "source": [
    "https://www.geeksforgeeks.org/python/python-word-embedding-using-word2vec/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b0b4f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will import necessary NLTK and Gensim for building the Word2Vec model and processing text:\n",
    "\n",
    "# (1) Word2Vec from gensim to build the word vector model.\n",
    "# (2) nltk.tokenize helps in splitting text into sentences and words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77afb2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment the line below to install gensim if not already installed\n",
    "# !pip install gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3ae45a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec Implementation\n",
    "import gensim\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "84f0f64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('learning', 0.19912248849868774), ('interesting', 0.1701892763376236), ('is', 0.14594517648220062)]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Example corpus\n",
    "sentences = [\n",
    "[\"I\", \"love\", \"machine\", \"learning\"],\n",
    "[\"Word2Vec\", \"is\", \"a\", \"interesting\", \"algorithm\"],\n",
    "[\"I\", \"enjoy\", \"coding\"] ]\n",
    "\n",
    "# Train a Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=1)\n",
    "# Get vector representation of a word\n",
    "vector = model.wv['Word2Vec']\n",
    "#print(vector)\n",
    "# Find most similar words to 'machine'\n",
    "similar_words = model.wv.most_similar('enjoy', topn=3)\n",
    "print(similar_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a66759f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word2Vec: “learn meaning by predicting context”\n",
    "#Idea:\n",
    "#      - You know a word by the company it keeps.\n",
    "\n",
    "# Word2Vec trains a tiny neural network to:\n",
    "#   - predict surrounding words from a target word (Skip-gram)\n",
    "#   - or predict a word from its context (CBOW)\n",
    "\n",
    "#Why it mattered\n",
    "# - First widely successful dense word embeddings\n",
    "# - Fast, simple, powerful\n",
    "# - Captured semantic relationships surprisingly well\n",
    "\n",
    "#Limitations\n",
    "# - Each word has one vector only (bank = river bank = money bank)\n",
    "# - Can’t handle out-of-vocabulary words\n",
    "# - Relies only on local context windows"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2483785c",
   "metadata": {},
   "source": [
    "## GloVe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05ba8879",
   "metadata": {},
   "source": [
    "https://medium.com/@mervebdurna/advanced-word-embeddings-word2vec-glove-and-fasttext-26e546ffedbd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e726f6f1",
   "metadata": {},
   "source": [
    "- GloVe is a model for learning word vectors based on the global co-occurrence\n",
    "matrix of words in a corpus.\n",
    "- Unlike Word2Vec, GloVe combines global and local information to capture the\n",
    "statistical information of word occurrences.\n",
    "- GloVe uses matrix factorization and captures global word co-occurrence statistics.\n",
    "- Key Idea: Words that appear in similar contexts will have similar vector\n",
    "representations, derived from the co-occurrence statistics.\n",
    "\n",
    "- GloVe is excellent for capturing global word relationships via co-occurrence matrices, but also faces\n",
    "limitations with OOV words\n",
    "\n",
    "- Use GloVe when you need to capture the global context and word relationships from the co-occurrence\n",
    "matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5af14b19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.4.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac6d297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api\n",
    "glove_model = api.load(\"glove-wiki-gigaword-50\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8953b71c-a775-4115-89cb-04187b668351",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Using GloVe Word Embeddings ---\n",
      "\n",
      "Most similar words to 'python':\n",
      "[('reticulated', 0.6916365623474121), ('spamalot', 0.6635736227035522), ('php', 0.6414496302604675), ('owl', 0.6301496028900146), ('mouse', 0.6275478601455688)]\n",
      "\n",
      "Most similar words to 'machine':\n",
      "[('machines', 0.8238813877105713), ('device', 0.8175780773162842), ('using', 0.7789539694786072), ('gun', 0.7508804798126221), ('used', 0.7492657899856567)]\n",
      "\n",
      "Analogy Example: king - man + woman ≈ ?\n",
      "[('queen', 0.8523604273796082), ('throne', 0.7664334177970886), ('prince', 0.7592144012451172), ('daughter', 0.7473883628845215), ('elizabeth', 0.7460219860076904)]\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "# -------------------------------\n",
    "# GloVe Embeddings\n",
    "# -------------------------------\n",
    "print(\"\\n--- Using GloVe Word Embeddings ---\")\n",
    "\n",
    "\n",
    "# Pick a word from the text\n",
    "word_choice = \"python\"\n",
    "if word_choice in glove_model:\n",
    "    print(f\"\\nMost similar words to '{word_choice}':\")\n",
    "    print(glove_model.most_similar(word_choice, topn=5))\n",
    "else:\n",
    "    print(f\"'{word_choice}' not in GloVe vocabulary\")\n",
    "\n",
    "# Explore another word\n",
    "word_choice2 = \"machine\"\n",
    "if word_choice2 in glove_model:\n",
    "    print(f\"\\nMost similar words to '{word_choice2}':\")\n",
    "    print(glove_model.most_similar(word_choice2, topn=5))\n",
    "\n",
    "# Word analogy example\n",
    "print(\"\\nAnalogy Example: king - man + woman ≈ ?\")\n",
    "print(glove_model.most_similar(positive=[\"king\", \"woman\"], negative=[\"man\"], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e626dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#GloVe: “count first, then learn”\n",
    "\n",
    "#Idea:\n",
    "#  - Instead of predicting words, GloVe uses global word co-occurrence statistics.\n",
    "\n",
    "#It looks at:\n",
    "# - How often does word A appear with word B across the entire corpus?\n",
    "# - Then learns vectors so that:\n",
    "# - ratios of co-occurrence probabilities encode meaning\n",
    "\n",
    "# Why GloVe exists\n",
    "\n",
    "# - Combines:\n",
    "#        - Word2Vec’s efficiency\n",
    "#        - traditional NLP’s global statistics\n",
    "# - Often gives more stable embeddings\n",
    "#- Strong on semantic structure\n",
    "\n",
    "# Trade-offs\n",
    "# - Still one vector per word\n",
    "# - Needs large corpora + preprocessing\n",
    "# - Less flexible than predictive models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9b5af5",
   "metadata": {},
   "source": [
    "## FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0b74c34",
   "metadata": {},
   "source": [
    "Use FastText when you need better handling of rare and unseen words, or for languages with rich morphology.\n",
    "- FastText, developed by Facebook AI Research (FAIR), is an extension of Word2Vec\n",
    "that represents each word as a bag of character n-grams.\n",
    "- This allows FastText to generate embeddings for out-of-vocabulary (OOV) words\n",
    "by breaking them down into n-grams.\n",
    "- FastText improves on Word2Vec by considering subword information (n-grams) for\n",
    "better handling of rare and out-of-vocabulary words.\n",
    "- Key Advantage: It performs better for morphologically rich languages and handles\n",
    "out-of-vocabulary words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6ff2e34b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.2919701e-03 -1.0602611e-04 -1.1323356e-03  1.6584302e-03\n",
      " -5.7117449e-04 -2.9840841e-04 -4.3193492e-04 -3.1250282e-04\n",
      " -1.9898350e-04  9.4852143e-04  1.6994212e-03 -1.8581563e-04\n",
      " -8.1228669e-04 -1.5968895e-03 -1.3839703e-03 -6.3576088e-05\n",
      " -5.7436171e-04 -1.1720147e-03  7.4763177e-04 -2.1684753e-05\n",
      "  4.5981101e-04 -1.7291495e-03 -4.4365969e-04  4.0478929e-04\n",
      "  1.2072949e-04  6.4071972e-04 -1.0785459e-03  1.3050955e-03\n",
      "  3.5044085e-04 -1.8284899e-04 -1.5951110e-04 -1.0465594e-03\n",
      " -1.5170674e-04 -5.7858619e-04 -1.8307484e-03  1.0248278e-03\n",
      "  6.9344341e-04  1.6159177e-03 -8.4400486e-04  8.9535897e-04\n",
      " -1.3508157e-04  2.3538095e-03 -3.7109022e-04 -2.8064058e-04\n",
      "  2.6269807e-04 -2.8326022e-04 -7.7332847e-04  1.8949938e-03\n",
      "  2.1798143e-03 -4.4569728e-04 -6.4175081e-04  1.4240020e-04\n",
      "  2.5182988e-03 -1.5666584e-03  1.3954224e-04 -6.9046958e-04\n",
      "  5.8793183e-04 -1.4282564e-03  2.1278318e-04 -2.2993281e-03\n",
      " -4.3249400e-03 -1.6397990e-03  1.3989839e-03 -1.3229308e-03\n",
      "  2.0258871e-03 -2.9663873e-04 -1.7821314e-03 -1.9624084e-04\n",
      " -3.8101443e-04  1.2712786e-03 -4.8752314e-05 -1.6701949e-03\n",
      " -4.0860524e-04 -2.3929863e-03 -1.1424356e-03 -8.1383681e-04\n",
      " -4.9799628e-04 -1.4722025e-03 -1.8545202e-03 -4.9134775e-04\n",
      "  2.1096619e-03 -3.5232501e-05  3.5016984e-03  1.4268994e-04\n",
      "  7.3083641e-04 -1.6456110e-03 -1.8567833e-03 -8.2786253e-04\n",
      "  2.2351660e-04  2.1772250e-03  1.3073307e-03  7.8967125e-05\n",
      "  2.6948133e-04 -2.0950858e-03  2.1080931e-03  4.0393372e-04\n",
      "  1.4223440e-03  6.8460318e-04  2.4361876e-03  5.3560670e-04]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Toy dataset\n",
    "sentences = [\"FastText embeddings handle subword information.\",\n",
    "             \"It is effective for various languages.\"]\n",
    "# Tokenize sentences\n",
    "tokenized_sentences = [word_tokenize(sentence.lower()) for sentence in sentences]\n",
    "\n",
    "# Train FastText model\n",
    "model = FastText(sentences=tokenized_sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access embeddings\n",
    "word_embeddings = model.wv\n",
    "print(word_embeddings['subword'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "208ed90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FastText: “words are made of pieces”\n",
    "\n",
    "# Idea:\n",
    "# - Words aren’t atomic — they’re built from character n-grams.\n",
    "\n",
    "# FastText:\n",
    "#  - represents a word as a sum of its subword vectors\n",
    "#  - learns embeddings for character chunks\n",
    "\n",
    "# Why FastText exists\n",
    "# - Handles:\n",
    "#      - rare words\n",
    "#      - misspellings\n",
    "#      - morphology (especially great for rich languages)\n",
    "# - Can generate vectors for words never seen before\n",
    "\n",
    "# Cost\n",
    "#  - Slightly bigger models\n",
    "#  - Still static embeddings (context-independent)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d9f9d33",
   "metadata": {},
   "source": [
    "All three (word2vec glove and fasttext) are now called static embeddings.\n",
    "\n",
    "Modern models like BERT, RoBERTa, GPT use:\n",
    "\n",
    "- contextual embeddings\n",
    "\n",
    "- the word bank changes vector depending on the sentence\n",
    "\n",
    "But:\n",
    "\n",
    "static embeddings are still:\n",
    "\n",
    "- cheaper\n",
    "\n",
    "- easier to train\n",
    "\n",
    "- great for small datasets or classic ML pipelines\n",
    "\n",
    "- foundational for understanding modern NLP"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
