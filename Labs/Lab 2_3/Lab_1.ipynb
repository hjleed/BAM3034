{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04dc196a",
   "metadata": {},
   "source": [
    "# One-Hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f3518195",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "# example1 = 'I  will eat the Pizza' # Sample set for our example\n",
    "example1 = 'I ate an apple and played the piano'\n",
    "vocab = []\n",
    "counter = 0\n",
    "                           \n",
    "for considered_word in example1.lower().split():\n",
    "    if considered_word not in vocab:\n",
    "        vocab.append((considered_word, counter+1))\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ea99b442",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('an', 3), ('and', 5), ('apple', 4), ('ate', 2), ('i', 1), ('piano', 8), ('played', 6), ('the', 7)]\n"
     ]
    }
   ],
   "source": [
    "print(sorted(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "6486816d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 5, 4, 2, 1, 8, 6, 7]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab = sorted(vocab)\n",
    "list = []\n",
    "for i in range(len(vocab)):\n",
    "    list.append(vocab[i][1])\n",
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "1bce424b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 5, 4, 2, 1, 8, 6, 7])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices_array = np.array(list)\n",
    "indices_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "16a961e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = np.zeros((indices_array.size, indices_array.max() + 1))\n",
    "result[np.arange(indices_array.size), indices_array] = 1\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed53ca0",
   "metadata": {},
   "source": [
    "## Bag Of Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93596b74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   good  great  learning  practice  start  welcome\n",
      "0     0      1         2         0      1        1\n",
      "1     1      0         1         1      0        0\n"
     ]
    }
   ],
   "source": [
    "####### Using CountVectorizer() #######\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "sentence1 = \"Welcome to Great Learning , Now start learning\"\n",
    "sentence2 = \"Learning is a good practice\"\n",
    "\n",
    "CountVec = CountVectorizer(stop_words='english')\n",
    "\n",
    "BagOfWords = CountVec.fit_transform([sentence1,sentence2])\n",
    " \n",
    "#create dataframe\n",
    "BagOfWords_DF = pd.DataFrame(BagOfWords.toarray(),columns=CountVec.get_feature_names_out())\n",
    "print(BagOfWords_DF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c25147cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['welcome great learning, start learning', 'learning good practice']\n",
      "['good' 'great' 'learning' 'practice' 'start' 'welcome']\n",
      "{'welcome': 5, 'great': 1, 'learning': 2, 'start': 4, 'good': 0, 'practice': 3}\n",
      "[[0 1 2 0 1 1]\n",
      " [1 0 1 1 0 0]]\n"
     ]
    }
   ],
   "source": [
    "####### Using CountVectorizer() and gensim library #######\n",
    "    ###    gensim is used to remove_stopwords    ###\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "\n",
    "text = [\"Welcome to Great Learning, Now start learning\",\"Learning is good practice\"]\n",
    "\n",
    "# Step - 1: Convert to lower case\n",
    "text = [i.lower() for i in text]\n",
    "\n",
    "\n",
    "# Step - 2: Stopword removal\n",
    "filtered_sentence = [remove_stopwords(t) for t in text]\n",
    "print(filtered_sentence)\n",
    "\n",
    "\n",
    "# Step - 3: Sentence Scoring\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Create sample set of documents\n",
    "docs = np.array(filtered_sentence)\n",
    "\n",
    "# Fit the bag-of-words model\n",
    "bag = vectorizer.fit_transform(docs)\n",
    "\n",
    "# Get unique words / tokens found in all the documents. The unique words / tokens represents the features\n",
    "print(vectorizer.get_feature_names_out())\n",
    "\n",
    "# Associate the indices with each unique word\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# Print the numerical feature vector\n",
    "print(bag.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "adf63362",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens for Sentence-1:  ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'learning']\n",
      "Tokens for Sentence-2:  ['learning', 'is', 'a', 'good', 'practice']\n",
      "Vocabulary(Unique Words):  ['welcome', 'to', 'great', 'learning', ',', 'now', 'start', 'is', 'a', 'good', 'practice']\n",
      "After Stop-word removal:  ['welcome', 'great', 'learning', 'now', 'start', 'good', 'practice']\n",
      "\n",
      "Bag-of-Words: \n",
      "[1, 1, 2, 1, 1, 0, 0]\n",
      "[0, 0, 1, 0, 0, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "####### From Scratch (or) Without using CountVectorizer() #######\n",
    "\n",
    "def vectorize(tokens):\n",
    "    vector=[]\n",
    "    for w in filtered_vocab:\n",
    "        vector.append(tokens.count(w))\n",
    "    return vector\n",
    "def unique(sequence):\n",
    "    seen = set()\n",
    "    unique = []\n",
    "    for x in sequence:\n",
    "        if not x in seen:\n",
    "            seen.add(x)\n",
    "            unique.append(x)\n",
    "    return unique\n",
    "\n",
    "stopwords=[\"to\",\"is\",\"a\"] # Stop-words given manually\n",
    "special_char=[\",\",\":\",\" \",\";\",\".\",\"?\"] # list of special characters\n",
    "\n",
    "sentence1 = \"Welcome to Great Learning , Now start learning\"\n",
    "sentence2 = \"Learning is a good practice\"\n",
    "\n",
    "#convert them to lower case\n",
    "sentence1 = sentence1.lower()\n",
    "sentence2 = sentence2.lower()\n",
    "\n",
    "#split the sentences into tokens\n",
    "tokens1 = sentence1.split()\n",
    "tokens2 = sentence2.split()\n",
    "\n",
    "print(\"Tokens for Sentence-1: \",tokens1)\n",
    "print(\"Tokens for Sentence-2: \",tokens2)\n",
    "\n",
    "#create a vocabulary list\n",
    "vocab=unique(tokens1+tokens2)\n",
    "print('Vocabulary(Unique Words): ',vocab)\n",
    "\n",
    "#filter the vocabulary list\n",
    "filtered_vocab=[]\n",
    "for w in vocab: \n",
    "    if w not in stopwords and w not in special_char: \n",
    "        filtered_vocab.append(w)\n",
    "        \n",
    "print('After Stop-word removal: ', filtered_vocab)\n",
    "#convert sentences into vectords\n",
    "vector1=vectorize(tokens1)\n",
    "vector2=vectorize(tokens2)\n",
    "print(\"\\nBag-of-Words: \")\n",
    "print(vector1)\n",
    "print(vector2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b14aee",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "5f5b578f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF Scores:\n",
      "    analyze  best  courses  data  fields  important  is  most  of  one  \\\n",
      "0        0     0        0     1       1          1   1     1   2    1   \n",
      "1        0     1        1     1       0          0   1     0   1    1   \n",
      "2        1     0        0     2       0          0   0     0   0    0   \n",
      "\n",
      "   science  scientists  the  this  \n",
      "0        2           0    1     0  \n",
      "1        1           0    1     1  \n",
      "2        0           1    0     0  \n",
      "\n",
      "Vocabulary:\n",
      " {'data': 3, 'science': 10, 'is': 6, 'one': 9, 'of': 8, 'the': 12, 'most': 7, 'important': 5, 'fields': 4, 'this': 13, 'best': 1, 'courses': 2, 'scientists': 11, 'analyze': 0}\n",
      "\n",
      "IDF Scores:  [1.69314718 1.69314718 1.69314718 1.         1.69314718 1.69314718\n",
      " 1.28768207 1.69314718 1.28768207 1.28768207 1.28768207 1.69314718\n",
      " 1.28768207 1.69314718]\n",
      "\n",
      "TF-IDF Scores:      analyze      best   courses      data    fields  important        is  \\\n",
      "0  0.000000  0.000000  0.000000  0.189526  0.320895   0.320895  0.244049   \n",
      "1  0.000000  0.400294  0.400294  0.236420  0.000000   0.000000  0.304434   \n",
      "2  0.542701  0.000000  0.000000  0.641055  0.000000   0.000000  0.000000   \n",
      "\n",
      "       most        of       one   science  scientists       the      this  \n",
      "0  0.320895  0.488098  0.244049  0.488098    0.000000  0.244049  0.000000  \n",
      "1  0.000000  0.304434  0.304434  0.304434    0.000000  0.304434  0.400294  \n",
      "2  0.000000  0.000000  0.000000  0.000000    0.542701  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "##### TF-IDF using TfidfVectorizer #####\n",
    "\n",
    "from sklearn.feature_extraction.text import  CountVectorizer, TfidfVectorizer\n",
    "\n",
    "docs = ['data science is one of the most important fields of science',\n",
    "        'this is one of the best data science courses',\n",
    "        'data scientists analyze data' ]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "word_count_vector = cv.fit_transform(docs)\n",
    "tf = pd.DataFrame(word_count_vector.toarray(), columns=cv.get_feature_names_out())\n",
    "print('TF Scores:\\n',tf)\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf = vectorizer.fit_transform(docs)\n",
    "vocab = vectorizer.vocabulary_ # Obtain the vocabulary and feature names\n",
    "idf = vectorizer.idf_ # Obtain the IDF scores\n",
    "\n",
    "tfidf_scores = tfidf.toarray()\n",
    "tfidf_scores = pd.DataFrame(tfidf_scores,columns=vectorizer.get_feature_names_out()) # Obtain the TF-IDF scores\n",
    "\n",
    "print('\\nVocabulary:\\n', vocab)\n",
    "print('\\nIDF Scores: ', idf)\n",
    "print('\\nTF-IDF Scores: ', tfidf_scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
