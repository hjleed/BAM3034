{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from random import shuffle\n",
    "import re\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import lxml.etree\n",
    "\n",
    "# extract subtitle\n",
    "doc = lxml.etree.parse(open('ted_en-20160408.xml', 'r'))\n",
    "\n",
    "input_text = '\\n'.join(doc.xpath('//content/text()'))\t\t\n",
    "\n",
    "# remove parenthesis\n",
    "input_text_noparens = re.sub(r'\\([^)]*\\)', '', input_text)\n",
    "\n",
    "# store as list of sentences\n",
    "sentences_strings_ted = []\n",
    "\n",
    "for line in input_text_noparens.split('\\n'):\n",
    "    m = re.match(r'^(?:(?P<precolon>[^:]{,20}):)?(?P<postcolon>.*)$', line)\n",
    "    sentences_strings_ted.extend(sent for sent in m.groupdict()['postcolon'].split('.') if sent)\n",
    "\n",
    "# store as list of lists of words\n",
    "sentences_ted = []\n",
    "for sent_str in sentences_strings_ted:\n",
    "    tokens = re.sub(r\"[^a-z0-9]+\", \" \", sent_str.lower()).split()\n",
    "    sentences_ted.append(tokens)\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model_ted = Word2Vec(sentences=sentences_ted, vector_size=100, window=5, min_count=5, workers=4, sg=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('woman', 0.8602633476257324),\n",
       " ('guy', 0.8204045295715332),\n",
       " ('lady', 0.7615416646003723),\n",
       " ('soldier', 0.7612980008125305),\n",
       " ('girl', 0.7477767467498779),\n",
       " ('boy', 0.7187429666519165),\n",
       " ('gentleman', 0.7056071758270264),\n",
       " ('kid', 0.6818613409996033),\n",
       " ('poet', 0.6656844615936279),\n",
       " ('person', 0.6572151184082031)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ted.wv.most_similar(\"man\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import FastText\n",
    "\n",
    "model_ted = FastText(sentences_ted, vector_size=100, window=5, min_count=5, workers=4,sg=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('arthritis', 0.8389511108398438),\n",
       " ('anthropocene', 0.8389402031898499),\n",
       " ('kp', 0.8350369334220886),\n",
       " ('pseudonym', 0.8325868248939514),\n",
       " ('curitiba', 0.8308171629905701),\n",
       " ('anagnorisis', 0.8261967301368713),\n",
       " ('karnataka', 0.823052167892456),\n",
       " ('pseudo', 0.8183250427246094),\n",
       " ('hirshhorn', 0.8172647953033447),\n",
       " ('aeronautics', 0.8166674971580505)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_ted.wv.most_similar(\"Gastroenteritis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Glove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "model = api.load(\"glove-wiki-gigaword-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('kick', 0.864284873008728),\n",
       " ('catch', 0.8190028667449951),\n",
       " ('off', 0.8133060336112976),\n",
       " ('kicking', 0.8079286813735962),\n",
       " ('got', 0.8033515214920044),\n",
       " ('throw', 0.7966356873512268),\n",
       " ('missed', 0.7893549799919128),\n",
       " ('back', 0.7857473492622375),\n",
       " ('throws', 0.7807802557945251),\n",
       " ('caught', 0.7794879674911499)]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(\"ball\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines= [\"Hello this is a tutorial to convert word to integer\" , \"It is a beautiful day\" , \"Jack is going to office\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'this', 'is', 'a', 'tutorial', 'to', 'convert', 'word', 'to', 'integer'], ['It', 'is', 'a', 'beautiful', 'day'], ['Jack', 'is', 'going', 'to', 'office']]\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "word_tokens=[]\n",
    "i = 0\n",
    "for line in lines:\n",
    "    words = word_tokenize(line)\n",
    "    word_tokens.insert(i,words)\n",
    "    i=i+1\n",
    "\n",
    "print (word_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello', 'tutorial', 'convert', 'word', 'integer', 'It', 'beautiful', 'day', 'Jack', 'going', 'office']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words=stopwords.words('english')\n",
    "\n",
    "lines_without_stopwords=[]\n",
    "for line in lines:\n",
    "    stop_removed=[]\n",
    "    for line in word_tokens:\n",
    "        for word in line:\n",
    "            if word not in stop_words:\n",
    "                stop_removed.append(word)\n",
    "\n",
    "print(stop_removed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['Hello', 'this', 'is', 'a', 'tutorial', 'to', 'convert', 'word', 'to', 'integer'], ['It', 'is', 'a', 'beautiful', 'day'], ['Jack', 'is', 'going', 'to', 'office']]\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer  \n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wordnet_lemmatizer = WordNetLemmatizer() \n",
    "\n",
    "lines_with_lemmas=[]\n",
    "for line in lines:\n",
    "    lem_line=[]\n",
    "    s = line.split() \n",
    "    for word in s:\n",
    "        if s not in stop_words: \n",
    "            lem_line.append(wordnet_lemmatizer.lemmatize(word)) \n",
    "    lines_with_lemmas.append(lem_line)\n",
    "\n",
    "print(lines_with_lemmas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing 30 training epochs with 4 threads\n",
      "Epoch 0\n",
      "Epoch 1\n",
      "Epoch 2\n",
      "Epoch 3\n",
      "Epoch 4\n",
      "Epoch 5\n",
      "Epoch 6\n",
      "Epoch 7\n",
      "Epoch 8\n",
      "Epoch 9\n",
      "Epoch 10\n",
      "Epoch 11\n",
      "Epoch 12\n",
      "Epoch 13\n",
      "Epoch 14\n",
      "Epoch 15\n",
      "Epoch 16\n",
      "Epoch 17\n",
      "Epoch 18\n",
      "Epoch 19\n",
      "Epoch 20\n",
      "Epoch 21\n",
      "Epoch 22\n",
      "Epoch 23\n",
      "Epoch 24\n",
      "Epoch 25\n",
      "Epoch 26\n",
      "Epoch 27\n",
      "Epoch 28\n",
      "Epoch 29\n"
     ]
    }
   ],
   "source": [
    "from glove import Corpus, Glove\n",
    "corpus = Corpus()\n",
    "\n",
    "corpus.fit(lines_with_lemmas, window=10)\n",
    "glove = Glove(no_components=5, learning_rate=0.05)\n",
    "glove.fit(corpus.matrix, epochs=30, no_threads=4, verbose=True)\n",
    "glove.add_dictionary(corpus.dictionary)\n",
    "\n",
    "glove.save('glove.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.08139673  0.03268511  0.07912026  0.05500959 -0.03326362]\n"
     ]
    }
   ],
   "source": [
    "print(glove.word_vectors[glove.dictionary['tutorial']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('going', 0.7350365882817204),\n",
       " ('Hello', 0.693252670358993),\n",
       " ('It', 0.6343277175485825),\n",
       " ('Jack', 0.4510978346710577)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove.most_similar('beautiful')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "d90c862db9876c4e17bd48d1acba965c6a6e89c4fbc44fe7b256ca5a0523bdaf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
