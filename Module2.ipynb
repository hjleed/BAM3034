{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a5875f40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".................\n",
      "Sentence Tokenization: ['The quick brown fox jumped over the lazy dog.']\n",
      ".................\n",
      "Word Tokenization: ['The', 'quick', 'brown', 'fox', 'jumped', 'over', 'the', 'lazy', 'dog', '.']\n"
     ]
    }
   ],
   "source": [
    "# Text Tokenization (Word and Sentence Tokenization) page(11)\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('punkt')\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "# Sentence Tokenization\n",
    "print(\".................\")\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Sentence Tokenization:\", sentences)\n",
    "# Word Tokenization\n",
    "print(\".................\")\n",
    "words = word_tokenize(text)\n",
    "print(\"Word Tokenization:\", words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ce98979",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'spellchecker'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# Text Correction (Spell Checking) page (12)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspellchecker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpellChecker\n\u001b[32m      3\u001b[39m \u001b[38;5;66;03m# Initialize the spell checker\u001b[39;00m\n\u001b[32m      4\u001b[39m spell = SpellChecker()\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'spellchecker'"
     ]
    }
   ],
   "source": [
    "# Text Correction (Spell Checking) page (12)\n",
    "from spellchecker import SpellChecker\n",
    "# Initialize the spell checker\n",
    "spell = SpellChecker()\n",
    "# Incorrect word example\n",
    "misspelled = \"quikc\"\n",
    "corrected_word = spell.correction(misspelled)\n",
    "print(f\"Corrected word for '{misspelled}':\", corrected_word)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f73480f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stemmed Words: ['run', 'run', 'runner', 'easili', 'fairli']\n"
     ]
    }
   ],
   "source": [
    "# Stemming   page (13)\n",
    "from nltk.stem import PorterStemmer\n",
    "# Initialize the Porter Stemmer\n",
    "ps = PorterStemmer()\n",
    "# Words to be stemmed\n",
    "words_to_stem = [\"running\", \"runs\", \"runner\", \"easily\", \"fairly\"]\n",
    "# Stem each word\n",
    "stemmed_words = [ps.stem(word) for word in words_to_stem]\n",
    "print(\"Stemmed Words:\", stemmed_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b91774db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatized Words (verbs): ['run', 'run', 'runner', 'easily', 'fairly']\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "# Download necessary NLTK data files\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "# Initialize the Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "# Words to be lemmatized\n",
    "words_to_lemmatize = [\"running\", \"runs\", \"runner\", \"easily\",\n",
    "\"fairly\"]\n",
    "# Lemmatize each word (as a verb in this case)\n",
    "lemmatized_words = [lemmatizer.lemmatize(word, pos='v') for\n",
    "word in words_to_lemmatize]\n",
    "print(\"Lemmatized Words (verbs):\", lemmatized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec97aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ▪ Handle contractions (I’m to I am, can’t to cannot)\n",
    "# ▪ Remove or replace specific words or phrases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c66775",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT'), ('quick', 'JJ'), ('brown', 'NN'), ('fox', 'NN'), ('jumps', 'VBZ'), ('over', 'IN'), ('the', 'DT'), ('lazy', 'JJ'), ('dog', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "# Code to implement POS \"Part of Speech\"\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Download necessary NLTK datasets \n",
    "#       (Activate the following 2 line if this is the 1st time)\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "# Sample text\n",
    "text = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Tokenize the text\n",
    "tokens = word_tokenize(text)\n",
    "# Perform POS tagging\n",
    "tagged_tokens = nltk.pos_tag(tokens)\n",
    "print(tagged_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "667bccee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON John/NNP)\n",
      "  (ORGANIZATION Doe/NNP)\n",
      "  is/VBZ\n",
      "  working/VBG\n",
      "  at/IN\n",
      "  (ORGANIZATION IBM/NNP)\n",
      "  in/IN\n",
      "  (GPE New/NNP York/NNP)\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "#Example of Shallow Parsing in Action (Python using NLTK):\n",
    "import nltk\n",
    "from nltk import pos_tag\n",
    "from nltk.chunk import ne_chunk\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Sample sentence\n",
    "sentence = \"John Doe is working at IBM in New York.\"\n",
    "# Tokenizing and POS tagging\n",
    "tokens = word_tokenize(sentence)\n",
    "tagged_tokens = pos_tag(tokens)\n",
    "# Shallow parsing using Named Entity Recognition (NER)\n",
    "chunked = ne_chunk(tagged_tokens)\n",
    "print(chunked)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97deede6",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block after function definition on line 7 (3352362039.py, line 9)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mdoc = nlp(sentence)\u001b[39m\n    ^\n\u001b[31mIndentationError\u001b[39m\u001b[31m:\u001b[39m expected an indented block after function definition on line 7\n"
     ]
    }
   ],
   "source": [
    "# Dependency Parsing using spaCy   page (37)\n",
    "# Importing spaCy\n",
    "import spacy\n",
    "# Load spaCy's English model for dependency parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Dependency Parsing using spaCy\n",
    "def dependency_parse(sentence):\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "print(\"Dependency Parsing Output:\")\n",
    "for token in doc:\n",
    "print(f\"{token.text} -> {token.dep_} -> {token.head.text}\")\n",
    "print()\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Perform Dependency Parsing\n",
    "dependency_parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9003c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constituency Parsing using spaCy    page (38)\n",
    "# Importing spaCy\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "# Load spaCy's English model for syntactic parsing\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "# Constituency Parsing (Tree visualization) using spaCy\n",
    "def constituency_parse(sentence):\n",
    "# Process the sentence\n",
    "doc = nlp(sentence)\n",
    "print(\"Constituency-like Tree Visualization:\")\n",
    "displacy.serve(doc, style=\"dep\", page=True)\n",
    "# Example sentence\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Perform Constituency-like Tree Visualization\n",
    "constituency_parse(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63d98d88",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
