{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cac4e56",
   "metadata": {},
   "source": [
    "## lab6-Tutorial Objective\n",
    "In this tutorial, you will learn how to:\n",
    "- Create a Bag of Words (BoW) model.\n",
    "- Generate N-Grams to capture relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac0cafb5",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "---\n",
    "\n",
    "### 1. Bag of Words (BoW)\n",
    "\n",
    "* **Key Insight:** **Presence & Prevalence.** * **The Logic:** If a word appears frequently, the document is likely about that topic.\n",
    "* **The Reality:** It treats text as an unordered \"soup.\" It captures the **vocabulary** used but completely ignores grammar, word order, and the relative importance of common words (like \"the\" or \"is\").\n",
    "\n",
    "### 2. TF-IDF\n",
    "\n",
    "* **Key Insight:** **Uniqueness & Significance.**\n",
    "* **The Logic:** A word is only \"important\" if it appears often in *one* document but rarely across the rest of the collection.\n",
    "* **The Reality:** It effectively filters out \"noise\" and automatically identifies **keywords**. It tells you what makes a specific document distinct from the crowd.\n",
    "\n",
    "### 3. Word Embeddings (e.g., Word2Vec)\n",
    "\n",
    "* **Key Insight:** **Similarity & Association.**\n",
    "* **The Logic:** \"You shall know a word by the company it keeps.\" Words used in similar environments are mathematically mapped close to one another.\n",
    "* **The Reality:** It captures **relationships** (e.g., *Paris* is to *France* as *Tokyo* is to *Japan*). However, it struggles with words that have multiple meanings depending on the sentence.\n",
    "\n",
    "### 4. Contextual Embeddings (e.g., BERT/Transformers)\n",
    "\n",
    "* **Key Insight:** **Nuance & Intent.**\n",
    "* **The Logic:** The meaning of a word is defined dynamically by every other word in the sentence.\n",
    "* **The Reality:** It understands **polysemy** (one word, multiple meanings) and complex structures like sarcasm or negation. It represents text as a living sequence rather than a static snapshot.\n",
    "\n",
    "---\n",
    "\n",
    "### Summary Comparison Table\n",
    "\n",
    "| Method | Level of Insight | Best For... |\n",
    "| --- | --- | --- |\n",
    "| **BoW** | Quantitative (How many?) | Basic keyword matching |\n",
    "| **TF-IDF** | Statistical (How unique?) | Search engines & document labeling |\n",
    "| **Word2Vec** | Semantic (How related?) | Finding synonyms & broad concepts |\n",
    "| **BERT** | Holistic (What's the intent?) | Translation, Q&A, & deep sentiment |\n",
    "\n",
    "--"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77b513e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Documents:\n",
      "Document 1: I love programming in Python.\n",
      "Document 2: Python is a great programming language.\n",
      "Document 3: I love coding in Python.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Sample corpus\n",
    "documents = [  \n",
    "    \"I love programming in Python.\",  \n",
    "    \"Python is a great programming language.\",  \n",
    "    \"I love coding in Python.\"  \n",
    "]\n",
    "\n",
    "print(\"Original Documents:\")\n",
    "for i, doc in enumerate(documents):\n",
    "    print(f\"Document {i+1}: {doc}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ae8f67-dc02-493c-9eff-cc58269b9154",
   "metadata": {},
   "source": [
    "## 1. Bag of Words (BoW)\n",
    "The Bag of Words model represents text data as a collection of word frequencies. It converts each document\n",
    "into a vector based on the frequency of words.\n",
    "### 1.1. Using CountVectorizer from scikit-learn\n",
    "Setup: Import the necessary modules and prepare the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9898206f-c487-4ba3-9287-b6078597113c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize CountVectorizer for BoW\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_bow = bow_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bow_df = pd.DataFrame(X_bow.toarray(), columns=bow_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c64ab1-7762-4e93-aad0-492c7bd64d8e",
   "metadata": {},
   "source": [
    "CountVectorizer creates a matrix in which each unique word is represented by a column of the matrix, and each text sample from the document is a row in the matrix. The value of each cell is nothing but the count of the word in that particular text sample.  This can be visualized as follows -"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c655be6-ab9b-4574-bb2a-8c5dc319ab9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words Model:\n",
      "   coding  great  in  is  language  love  programming  python\n",
      "0       0      0   1   0         0     1            1       1\n",
      "1       0      1   0   1         1     0            1       1\n",
      "2       1      0   1   0         0     1            0       1\n",
      "............Explanation............\n",
      "• Each row represents a document.\n",
      "\n",
      "• Each column represents a word from the corpus.\n",
      "\n",
      "• The values in the matrix represent the frequency of the words in the corresponding\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Bag of Words Model:\")\n",
    "print(bow_df)\n",
    "#print(f\"\\nVocabulary: {list(bow_vectorizer.get_feature_names_out())}\")\n",
    "print(\"............Explanation............\")\n",
    "print(\"• Each row represents a document.\\n\")\n",
    "print(\"• Each column represents a word from the corpus.\\n\")\n",
    "print(\"• The values in the matrix represent the frequency of the words in the corresponding\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dce14ad-cd93-4ef3-b092-bc88445cd806",
   "metadata": {},
   "source": [
    "## N-Grams are contiguous sequences of n words from a given text. For example:\n",
    "-  1-Grams: Single words (unigrams).\n",
    "-  2-Grams: Pairs of words (bigrams).\n",
    "-  3-Grams: Triples of words (trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6f210d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "2. N-GRAMS MODELS\n",
      "==================================================\n",
      "\n",
      "2.1 BIGRAMS (2-GRAMS)\n",
      "--------------------\n",
      "Bigrams Model:\n",
      "   coding in  great programming  in python  is great  love coding  \\\n",
      "0          0                  0          1         0            0   \n",
      "1          0                  1          0         1            0   \n",
      "2          1                  0          1         0            1   \n",
      "\n",
      "   love programming  programming in  programming language  python is  \n",
      "0                 1               1                     0          0  \n",
      "1                 0               0                     1          1  \n",
      "2                 0               0                     0          0  \n",
      "\n",
      "Bigram features: ['coding in', 'great programming', 'in python', 'is great', 'love coding', 'love programming', 'programming in', 'programming language', 'python is']\n",
      "Number of bigram features: 9\n",
      "\n",
      "2.2 TRIGRAMS (3-GRAMS)\n",
      "--------------------\n",
      "Trigrams Model:\n",
      "   coding in python  great programming language  is great programming  \\\n",
      "0                 0                           0                     0   \n",
      "1                 0                           1                     1   \n",
      "2                 1                           0                     0   \n",
      "\n",
      "   love coding in  love programming in  programming in python  python is great  \n",
      "0               0                    1                      1                0  \n",
      "1               0                    0                      0                1  \n",
      "2               1                    0                      0                0  \n",
      "\n",
      "Trigram features: ['coding in python', 'great programming language', 'is great programming', 'love coding in', 'love programming in', 'programming in python', 'python is great']\n",
      "Number of trigram features: 7\n",
      "\n",
      "2.3 MIXED N-GRAMS (1-3 GRAMS)\n",
      "-------------------------\n",
      "Mixed N-grams Model (1-3 grams):\n",
      "   coding  coding in  coding in python  great  great programming  \\\n",
      "0       0          0                 0      0                  0   \n",
      "1       0          0                 0      1                  1   \n",
      "2       1          1                 1      0                  0   \n",
      "\n",
      "   great programming language  in  in python  is  is great  ...  \\\n",
      "0                           0   1          1   0         0  ...   \n",
      "1                           1   0          0   1         1  ...   \n",
      "2                           0   1          1   0         0  ...   \n",
      "\n",
      "   love coding in  love programming  love programming in  programming  \\\n",
      "0               0                 1                    1            1   \n",
      "1               0                 0                    0            1   \n",
      "2               1                 0                    0            0   \n",
      "\n",
      "   programming in  programming in python  programming language  python  \\\n",
      "0               1                      1                     0       1   \n",
      "1               0                      0                     1       1   \n",
      "2               0                      0                     0       1   \n",
      "\n",
      "   python is  python is great  \n",
      "0          0                0  \n",
      "1          1                1  \n",
      "2          0                0  \n",
      "\n",
      "[3 rows x 24 columns]\n",
      "\n",
      "Number of mixed n-gram features: 24\n"
     ]
    }
   ],
   "source": [
    "# 2. N-Grams Models\n",
    "print(\"=\" * 50)\n",
    "print(\"2. N-GRAMS MODELS\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# 2.1 Bigrams (2-grams)\n",
    "print(\"\\n2.1 BIGRAMS (2-GRAMS)\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Initialize CountVectorizer for bigrams\n",
    "bigram_vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_bigram = bigram_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "bigram_df = pd.DataFrame(X_bigram.toarray(), columns=bigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Bigrams Model:\")\n",
    "print(bigram_df)\n",
    "print(f\"\\nBigram features: {list(bigram_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Number of bigram features: {len(bigram_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# 2.2 Trigrams (3-grams)\n",
    "print(\"\\n2.2 TRIGRAMS (3-GRAMS)\")\n",
    "print(\"-\" * 20)\n",
    "\n",
    "# Initialize CountVectorizer for trigrams\n",
    "trigram_vectorizer = CountVectorizer(ngram_range=(3, 3))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_trigram = trigram_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "trigram_df = pd.DataFrame(X_trigram.toarray(), columns=trigram_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Trigrams Model:\")\n",
    "print(trigram_df)\n",
    "print(f\"\\nTrigram features: {list(trigram_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"Number of trigram features: {len(trigram_vectorizer.get_feature_names_out())}\")\n",
    "\n",
    "# 2.3 Mixed N-grams (1-3 grams)\n",
    "print(\"\\n2.3 MIXED N-GRAMS (1-3 GRAMS)\")\n",
    "print(\"-\" * 25)\n",
    "\n",
    "# Initialize CountVectorizer for mixed n-grams (unigrams, bigrams, trigrams)\n",
    "mixed_vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
    "\n",
    "# Fit and transform the corpus\n",
    "X_mixed = mixed_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Convert to DataFrame\n",
    "mixed_df = pd.DataFrame(X_mixed.toarray(), columns=mixed_vectorizer.get_feature_names_out())\n",
    "\n",
    "print(\"Mixed N-grams Model (1-3 grams):\")\n",
    "print(mixed_df)\n",
    "print(f\"\\nNumber of mixed n-gram features: {len(mixed_vectorizer.get_feature_names_out())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f057e72-7066-4834-b0fb-f98120b50412",
   "metadata": {},
   "source": [
    "## Summary\n",
    "- Bag of Words: A representation of text data where each document is represented by word\n",
    "frequencies.\n",
    "- N-Grams: Sequences of n words that capture contextual relationships between words (unigrams,\n",
    "bigrams, trigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1d1b6293-3041-436a-807f-4ac46f863236",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "SUMMARY\n",
      "==================================================\n",
      "• Bag of Words features: 8\n",
      "• Bigram features: 9\n",
      "• Trigram features: 7\n",
      "• Mixed N-gram features: 24\n",
      "\n",
      "Note: As n increases, the feature space grows exponentially!\n"
     ]
    }
   ],
   "source": [
    "# Summary\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"• Bag of Words features: {len(bow_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"• Bigram features: {len(bigram_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"• Trigram features: {len(trigram_vectorizer.get_feature_names_out())}\")\n",
    "print(f\"• Mixed N-gram features: {len(mixed_vectorizer.get_feature_names_out())}\")\n",
    "print(\"\\nNote: As n increases, the feature space grows exponentially!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5fcd83e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Corpus 1 – Movies (unigrams) ===\n",
      "Documents:\n",
      "1. This movie was absolutely fantastic and thrilling.\n",
      "2. The plot was boring and the acting felt fake.\n",
      "3. I loved the visuals but hated the ending.\n",
      "4. Best action film I've seen in years!\n",
      "\n",
      "Vocabulary size: 25\n",
      "   absolutely  acting  action  and  best  boring  but  ending  fake  \\\n",
      "0           1       0       0    1     0       0    0       0     0   \n",
      "1           0       1       0    1     0       1    0       0     1   \n",
      "2           0       0       0    0     0       0    1       1     0   \n",
      "3           0       0       1    0     1       0    0       0     0   \n",
      "\n",
      "   fantastic  ...  movie  plot  seen  the  this  thrilling  ve  visuals  was  \\\n",
      "0          1  ...      1     0     0    0     1          1   0        0    1   \n",
      "1          0  ...      0     1     0    2     0          0   0        0    1   \n",
      "2          0  ...      0     0     0    2     0          0   0        1    0   \n",
      "3          0  ...      0     0     1    0     0          0   1        0    0   \n",
      "\n",
      "   years  \n",
      "0      0  \n",
      "1      0  \n",
      "2      0  \n",
      "3      1  \n",
      "\n",
      "[4 rows x 25 columns]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "=== Corpus 1 – Movies (bigrams) ===\n",
      "Documents:\n",
      "1. This movie was absolutely fantastic and thrilling.\n",
      "2. The plot was boring and the acting felt fake.\n",
      "3. I loved the visuals but hated the ending.\n",
      "4. Best action film I've seen in years!\n",
      "\n",
      "Vocabulary size: 26\n",
      "   absolutely fantastic  acting felt  action film  and the  and thrilling  \\\n",
      "0                     1            0            0        0              1   \n",
      "1                     0            1            0        1              0   \n",
      "2                     0            0            0        0              0   \n",
      "3                     0            0            1        0              0   \n",
      "\n",
      "   best action  boring and  but hated  fantastic and  felt fake  ...  seen in  \\\n",
      "0            0           0          0              1          0  ...        0   \n",
      "1            0           1          0              0          1  ...        0   \n",
      "2            0           0          1              0          0  ...        0   \n",
      "3            1           0          0              0          0  ...        1   \n",
      "\n",
      "   the acting  the ending  the plot  the visuals  this movie  ve seen  \\\n",
      "0           0           0         0            0           1        0   \n",
      "1           1           0         1            0           0        0   \n",
      "2           0           1         0            1           0        0   \n",
      "3           0           0         0            0           0        1   \n",
      "\n",
      "   visuals but  was absolutely  was boring  \n",
      "0            0               1           0  \n",
      "1            0               0           1  \n",
      "2            1               0           0  \n",
      "3            0               0           0  \n",
      "\n",
      "[4 rows x 26 columns]\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "=== Corpus 2 – Products (1–2 grams) ===\n",
      "Documents:\n",
      "1. The battery lasts all day and charges quickly.\n",
      "2. Really disappointed with the build quality.\n",
      "3. Great value for money, highly recommend.\n",
      "4. Screen is bright but speakers are weak.\n",
      "5. Perfect size and very lightweight.\n",
      "\n",
      "Vocabulary size: 57\n",
      "   all  all day  and  and charges  and very  are  are weak  battery  \\\n",
      "0    1        1    1            1         0    0         0        1   \n",
      "1    0        0    0            0         0    0         0        0   \n",
      "2    0        0    0            0         0    0         0        0   \n",
      "3    0        0    0            0         0    1         1        0   \n",
      "4    0        0    1            0         1    0         0        0   \n",
      "\n",
      "   battery lasts  bright  ...  the  the battery  the build  value  value for  \\\n",
      "0              1       0  ...    1            1          0      0          0   \n",
      "1              0       0  ...    1            0          1      0          0   \n",
      "2              0       0  ...    0            0          0      1          1   \n",
      "3              0       1  ...    0            0          0      0          0   \n",
      "4              0       0  ...    0            0          0      0          0   \n",
      "\n",
      "   very  very lightweight  weak  with  with the  \n",
      "0     0                 0     0     0         0  \n",
      "1     0                 0     0     1         1  \n",
      "2     0                 0     0     0         0  \n",
      "3     0                 0     1     0         0  \n",
      "4     1                 1     0     0         0  \n",
      "\n",
      "[5 rows x 57 columns]\n",
      "----------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "def show_vectorization(name, docs, ngram_range=(1,1)):\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Documents:\")\n",
    "    for i, d in enumerate(docs, 1):\n",
    "        print(f\"{i}. {d}\")\n",
    "    \n",
    "    vec = CountVectorizer(ngram_range=ngram_range)\n",
    "    X = vec.fit_transform(docs)\n",
    "    df = pd.DataFrame(X.toarray(), columns=vec.get_feature_names_out())\n",
    "    \n",
    "    print(f\"\\nVocabulary size: {len(vec.get_feature_names_out())}\")\n",
    "    print(df)\n",
    "    print(\"-\"*70)\n",
    "\n",
    "# Example usage\n",
    "corpus1 = [\n",
    "    \"This movie was absolutely fantastic and thrilling.\",\n",
    "    \"The plot was boring and the acting felt fake.\",\n",
    "    \"I loved the visuals but hated the ending.\",\n",
    "    \"Best action film I've seen in years!\"\n",
    "]\n",
    "\n",
    "corpus2 = [\n",
    "    \"The battery lasts all day and charges quickly.\",\n",
    "    \"Really disappointed with the build quality.\",\n",
    "    \"Great value for money, highly recommend.\",\n",
    "    \"Screen is bright but speakers are weak.\",\n",
    "    \"Perfect size and very lightweight.\"\n",
    "]\n",
    "\n",
    "# Run different ranges\n",
    "show_vectorization(\"Corpus 1 – Movies (unigrams)\", corpus1, (1,1))\n",
    "show_vectorization(\"Corpus 1 – Movies (bigrams)\", corpus1, (2,2))\n",
    "show_vectorization(\"Corpus 2 – Products (1–2 grams)\", corpus2, (1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4a6a76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1143d1d4",
   "metadata": {},
   "source": [
    "For tutorial Lab6, here are the key insights from each text representation method and a reflection on how these choices impact analysis.\n",
    "\n",
    "### **Key Insights from Text Representations**\n",
    "\n",
    "* **Bag of Words (BoW):**\n",
    "* **- Insight:** Focuses on Presence & Prevalence, assuming that frequent words indicate the document's topic.\n",
    "* **- Core Idea:** It uses simple word counts or presence.\n",
    "* **- Reality:** It treats text as an unordered \"soup,\" capturing vocabulary but completely ignoring grammar, word order, and the importance of common words like \"the\" or \"is\".\n",
    "\n",
    "\n",
    "* **TF-IDF (Term Frequency-Inverse Document Frequency):**\n",
    "* **- Insight:** Focuses on Uniqueness & Significance.\n",
    "* **- Core Idea:** It down-weights common words by multiplying term frequency by inverse document frequency.\n",
    "* **- Reality:** It effectively identifies keywords that make a document distinct while filtering out \"noise,\" though it still ignores word order.\n",
    "\n",
    "\n",
    "* **Word Embeddings (e.g., Word2Vec):**\n",
    "* **- Insight:** Focuses on Similarity & Association based on the distributional hypothesis (\"you shall know a word by the company it keeps\").\n",
    "* **- Core Idea:** Uses static embeddings to map words used in similar environments close together mathematically.\n",
    "* **- Reality:** It captures semantic relationships and analogies but struggles with context and polysemy (words with multiple meanings).\n",
    "\n",
    "\n",
    "* **Contextual Embeddings (e.g., BERT/Transformers):**\n",
    "* **- Insight:** Focuses on Nuance & Intent.\n",
    "* **- Core Idea:** The meaning of a word is defined dynamically by every other word in the sentence.\n",
    "* **- Reality:** It understands complex structures like sarcasm, negation, and polysemy, though it is computationally heavy.\n",
    "\n",
    "\n",
    "* **N-Grams:**\n",
    "* **- Insight:** Captures contextual relationships between words by looking at contiguous sequences of *n* words (unigrams, bigrams, trigrams).\n",
    "* **- Reality:** While they provide more context than single words, the feature space grows exponentially as *n* increases.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### **Reflection: Impact of Choice on Analysis**\n",
    "\n",
    "The choice of text representation serves as a filter that determines what information your analysis can actually \"see.\"\n",
    "\n",
    "1. **From Counting to Understanding:** Choosing **BoW** or **TF-IDF** limits the analysis to a quantitative or statistical level (keyword matching), whereas BERT allows for a holistic analysis of intent and meaning.\n",
    "2. **Trade-off between Speed and Depth:** BoW is very simple and fast, making it ideal for baselines or keyword-based tasks. In contrast, BERT is computationally heavy but necessary for modern NLP tasks like Question Answering (QA) or deep sentiment analysis.\n",
    "3. **Handling Ambiguity:** If your analysis involves words with multiple meanings (polysemy), static methods like Word2Vec will fail to distinguish them, whereas contextual embeddings like **BERT** represent text as a \"living sequence\" that adapts to the surrounding words.\n",
    "4. **Feature Explosion:** Choosing to use N-Grams to capture context can significantly increase the complexity of the data; for example, a small corpus might have only 8 BoW features but jump to 24 features when using mixed 1-3 grams."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BAM3034",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
